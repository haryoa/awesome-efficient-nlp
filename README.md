# Awesome Efficient and Low-Resource NLP

This GitHub will list several papers that align with my interests. I will post a summary on my blog in the future.

(Note: This list is still in progress. The list might be organized, yet in messy order.)

## Language Model Training Objective 

- ![Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737)
- ![Frustratingly Simple Pretraining Alternatives to Masked Language Modeling](https://arxiv.org/abs/2109.01819)
- ![Pre-Training Transformers as Energy-Based Cloze Models](https://arxiv.org/abs/2012.08561)
- ![ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)

## Knowledge Distillation
- ![Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection](https://aclanthology.org/2022.findings-emnlp.464/)
- ![Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method](https://aclanthology.org/2023.findings-acl.614/)

## Efficient Architecture Tweak
- ![Alternating Updates for Efficient Transformers](https://arxiv.org/abs/2301.13310)


## Continual learning
- ![Amortizing intractable inference in large language models](https://openreview.net/forum?id=Ouj6p4ca60)
- 

## Nice Addition
- ![A Cookbook of Self-Supervised Learning](https://arxiv.org/abs/2304.12210)
- ![Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning](https://arxiv.org/abs/2012.09816)
- ![What Knowledge Gets Distilled in Knowledge Distillation?](https://arxiv.org/abs/2205.16004)
