# Awesome Efficient and Low-Resource NLP

This GitHub will list several papers that align with my interests. I will post a summary on my blog in the future.

(Note: This list is still in progress. The list might be organized, yet in messy order.)

## Language Model Training Objective 

- [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737)
- [Should You Mask 15% in Masked Language Modeling?](https://arxiv.org/abs/2202.08005)
- [Frustratingly Simple Pretraining Alternatives to Masked Language Modeling](https://arxiv.org/abs/2109.01819)
- [Pre-Training Transformers as Energy-Based Cloze Models](https://arxiv.org/abs/2012.08561)
- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)
- [How does the pre-training objective affect what large language models learn about linguistic properties?](https://arxiv.org/abs/2203.10415)
- [How does the task complexity of masked pretraining objectives affect downstream performance?](https://arxiv.org/abs/2305.10992)
- [How does the pre-training objective affect what large language models learn about linguistic properties?](https://arxiv.org/abs/2310.17271)
- [On the utility of enhancing BERT syntactic bias with Token Reordering Pretraining](https://arxiv.org/abs/2203.10415)
- [Instance Regularization for Discriminative Language Model Pre-training](https://arxiv.org/abs/2210.05471)
- [Language model pre-training on true negatives](https://ojs.aaai.org/index.php/AAAI/article/view/26639)
  
  
## Knowledge Distillation
- [Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection](https://aclanthology.org/2022.findings-emnlp.464/)
- [Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method](https://aclanthology.org/2023.findings-acl.614/)

## Efficient Architecture Tweak
- [Alternating Updates for Efficient Transformers](https://arxiv.org/abs/2301.13310)


## Continual learning
- [Amortizing intractable inference in large language models](https://openreview.net/forum?id=Ouj6p4ca60)
- [Cross-lingual Continual Learning](https://aclanthology.org/2023.acl-long.217/)

## Nice Addition
- [A Cookbook of Self-Supervised Learning](https://arxiv.org/abs/2304.12210)
- [Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning](https://arxiv.org/abs/2012.09816)
- [What Knowledge Gets Distilled in Knowledge Distillation?](https://arxiv.org/abs/2205.16004)
