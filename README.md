# Awesome Efficient and Low-Resource NLP

This GitHub will list several papers that align with my interests. I will post a summary on my blog in the future.

(Note: This list is still in progress)

## Language Model Training Objective 

- ![Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737)
- ![Frustratingly Simple Pretraining Alternatives to Masked Language Modeling](https://arxiv.org/abs/2109.01819)
- ![Pre-Training Transformers as Energy-Based Cloze Models](https://arxiv.org/abs/2012.08561)
- ![ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)

## Knowledge Distillation

- ![Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection](https://aclanthology.org/2022.findings-emnlp.464/)
- 

## Nice Addition
- ![A Cookbook of Self-Supervised Learning](https://arxiv.org/abs/2304.12210)
- 
